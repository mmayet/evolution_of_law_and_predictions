{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TFIDF` Eras\n",
    "\n",
    "The goal is to see how key words and phrases have changed and evolved over time. Are the schools stagnant in the way they address law? Are their new methods or view points? How do the individual schools differ, and do they evolve in the same way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from pyarabic import araby\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Up Eras\n",
    "To split up books by era, the dict `time_ranges` defines how many books are in each of the 6 eras. Initially, I used 4 eras, but after a few trials and more careful consideration, I decided to go with 6.\n",
    "\n",
    "In general, this is how the time frames are split up:\n",
    "\n",
    "| Era |  Time Range |                      |\n",
    "|:---:|:-----------:|:--------------------:|\n",
    "|  0  | 0000 - 0200 | Initial Codification |\n",
    "|  1  | 0200 - 0500 |    Schools Spread    |\n",
    "|  2  | 0500 - 0700 |  Major Global Works  |\n",
    "|  3  | 0700 - 0900 |     Commentaries     |\n",
    "|  4  | 0900 - 1250 |  Indstrl Rev & Euro  |\n",
    "|  5  | 1250 - 1450 |     Modern Times     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['134','135','136','137']\n",
    "\n",
    "time_ranges = {\n",
    "    134: [7, 6, 10, 10, 10, 5],\n",
    "    135: [2, 18, 12, 8, 17, 9],\n",
    "    136: [2, 8, 15, 10, 16, 9],     # [1, 9, 15, 10, 16, 9]   Used time w/ al-Muzanī because of min_df in TFIDF \n",
    "    137: [6, 10, 12, 12, 20, 17]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `texts_13*.json` vs `texts_13*_stemmed.json`\n",
    "I looked at `word frequencies` with the same text stemmed with NLTK and not stemmed, and found that NLTK has many issues when it comes to properly stemming, thus I totally dropped that. Because of that, stemming had a large negative effect on the `word frequencies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books = []\n",
    "for category_id in categories:\n",
    "    with open('gensim_files/texts_'+str(category_id)+'.json') as f:\n",
    "        books = json.load(f)\n",
    "    all_books.append(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw1 = get_stop_words('arabic') + stopwords.words(\"arabic\")\n",
    "sw2 = ['ا','أ','إ','ذ','ض','ص','ث','ق','ف','غ','ع','ه','خ','ح','ج','ش','س','ي','ب','ل','ا','ال','ت','ن','م','ك','ئ','ء','ؤ','ر','لا','ى','ة','و','ز','ظ']\n",
    "sw3 = [\"آله\", \"أبو\", \"أبي\", \"أثنى\", \"أحدهما\", \"أظهرهما\", \"أن\", \"أنه\", \"أو\", \"أى\", \"إلا\", \"إلخ\", \"إله\", \"إلى\", \"ابن\", \"ابو\",\"ابي\", \"الآتي\", \"الأستاذ\", \"الأولباب\", \"الامام\", \"البصير\", \"الثانية\", \"الجلال\", \"الحمد\", \"الخ\", \"الدكتور\", \"الرحمن\", \"الرحيم\", \"الرسول\", \"السميع\",\"الشارح\", \"الشيء\", \"الشيخ\", \"الصمد\", \"العبد\", \"العلامة\", \"العلي\", \"الفقير\", \"القدير\", \"الكتاب\", \"الله\", \"المؤلف\", \"المؤلفة\", \"المجلد\",\"المسألة\", \"المصنف\", \"النبي\", \"الي\", \"انتهى\", \"انظر\", \"اهـ\", \"باب\", \"بالواو\", \"بدلالة\", \"برقم\", \"بسم\", \"بعد\", \"بقيد\", \"بمثابة\", \"بن\", \"به\",\"بها\", \"بين\", \"بينهما\", \"تأمل\", \"تخريجه\", \"تعالى\", \"تعبيره\", \"ثم\", \"ثناء\", \"حتى\", \"حكاهما\", \"حمدا\", \"خصما\", \"دليلنا\", \"ذكرنا\", \"ذكرناه\", \"ر\"]\n",
    "sw4 = ['لأنهما','يحصل','قولهما','بدون','','وأنه','وروى','المقصود','أصلا','لوجود','الشرع','فقط','ولعل','اختلافهم','فقولان','فصلوأما','اليه','قدمنا','ثالثها','معنى','تقدم','والله','أنها','بلا','وأن','بفاس','منهم','أعلم','ففيه','وهل','أنها','وأن','ذكره','كلامه','قاله','نقله','منهما','بأنه','بنحو','ومحل','نقل','وجهين','فعلى','كون','وأن','أحد','بلا','','','','','','لقول','أنها','أخذ','ففي','ذكره','فاذا','ويدل','قيل','قالوا','القول','وجه','المعنى','وجهين','باعتبار','اعتبار','بينا','بأس','فلذلك','فلهذا','وقاله','تأويلان','القولين','بقوله','إليه','بذلك','شيئا','عنده','وذاك','لعدم','ومنهم','قولين','عبارة','زيادتي','وينبغي','ولهذا','أكان','وخبر','وحينئذ','رحمهم','فهنا','إليه','فلم','غيره','أيضا','ولسنا','جميعهم','وليسوا','الأوجه','التالية','وثالثها','قلت','وكذلك','وسلم','وقال','شيء','لأن','لأن','فهو','فقال','لأنه','رحمه','فلما','يكن','وابن','رسول','النبي','وقيل','وكذا','وإلا','ونحوه','واحد','فلو','الأول','بأن','والثاني','وجهان','قلنا','الله',\"فقال\",\"وعن\",\"ربه\", \"رحمة\", \"رسول\", \"رضى\", \"رضي\", \"رقم\", \"رها\", \"سبحانه\", \"سنن\", \"سيأتي\", \"شرح\", \"شيخ\", \"صلى\", \"طبقات\", \"عبد\", \"على\", \"عليكم\", \"عليه\", \"عن\",\"عند\", \"عنه\", \"غير\", \"فأشبه\", \"فأما\", \"فإن\", \"فإنا\", \"فإنه\", \"فائدة\", \"فافهم\", \"فالأصح\", \"فالقاضي\", \"فالوجه\", \"فان\", \"فانه\", \"فجاز\", \"فدل\", \"فصل\",\"فكان\", \"فلأن\", \"فلأنه\", \"فلا\", \"فلما\", \"فليتأمل\", \"فمنهم\", \"فنقول\", \"فهذا\", \"فهل\", \"فوجهان\", \"فى\", \"في\", \"فيه\", \"فيها\", \"قال\", \"قبل\", \"قدمناه\",\"قلنا\", \"قول\", \"قولان\", \"قوله\", \"كان\", \"كتاب\", \"كلام\", \"كلامهم\", \"كما\", \"كونه\", \"لأنا\", \"لأنه\", \"لأنها\", \"لان\", \"لانه\", \"لخبر\", \"لذلك\", \"لرحمة\",\"لقوله\", \"له\", \"لهم\", \"لو\", \"مادة\", \"مثال\", \"مثلا\", \"عبدا\", \"مع\", \"معطوف\", \"مقدمة\", \"من\", \"مناهج\", \"منتهى\", \"منه\", \"نسخة\", \"نسلم\", \"نصا\",\"نصه\", \"نقول\", \"هريرة\", \"ههنا\", \"وأصحهما\", \"وأما\", \"وأيضا\", \"وإن\", \"وإنما\", \"وإنه\", \"واحتج\", \"واعلم\", \"والتقوى\", \"والثانى\", \"والثاني\",\"والسلام\", \"والصلاة\", \"وان\", \"وانظر\", \"وبالله\", \"وبه\", \"وتقدم\", \"وجزم\", \"وجل\", \"وجهان\", \"وسلم\", \"وسن\", \"وشرعا\", \"وصلى\", \"وعبارة\", \"وعلى\",\"وعنه\", \"وغيره\", \"وغيرهم\", \"وفى\", \"وقال\", \"وقد\", \"وقدمه\", \"وقوله\", \"وقولي\", \"وقيل\", \"وكذلك\", \"ولأن\", \"ولأنه\", \"ولأنها\", \"ولذا\", \"ولكنا\", \"ولنا\", \"ولو\",\"عنهم\", \"وهذا\", \"وهنا\",'فذا','فهذ','هتحقيق','فاستخلفه','واعتبارا','وبالجملة','اليها','وذا',\"وهو\", \"ويحتمل\", \"يعني\", \"يقال\", \"يقول\", \"يكون\"]\n",
    "sw = set(sw1+sw2+sw3+sw4)\n",
    "    \n",
    "def dummy(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TFIDF Vectorizer`\n",
    "Now, it's important to know what TfidfVectorizer does. It looks at term frequency within a document.\n",
    "\n",
    "Term frequency (tf) is the frequency of a certain term in a document:\n",
    "\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = \\frac{N_\\text{term}}{N_\\text{terms in Document}}\n",
    "$$\n",
    "where\n",
    "\n",
    "$N_\\text{term}$ is the number of times a term/word $t$ appears in document $d$\n",
    "$N_\\text{terms in Document}$ is the number of terms/words in document $d$\n",
    "Inverse document frequency (idf) is defined as the frequency of documents that contain that term over the whole corpus:\n",
    "\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = \\log\\frac{N_\\text{Documents}}{N_\\text{Documents that contain term}}\n",
    "$$\n",
    "where\n",
    "\n",
    "- $N_\\text{Documents}$ is the number of documents in the corpus $D$\n",
    "- $N_\\text{Documents that contain term}$ is the number of documents in $D$ that contain term/word $t$\n",
    "TF-IDF is then calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n",
    "The results penalizes common words, and rare words have more weight. The fact is, this is much more robust and methodical vectorizer than Count or Hash, which is why I didn't even bother to use the others. It helps the words from each subreddit stand out with much more weight.\n",
    "\n",
    "## `Tuning`\n",
    "- Since 2 schools have only `2` books `min_df` had to be set to `1`.\n",
    "- Since many texts are filled with many concepts of law, quoting previous scholars, there is **a lot** of repetition. Thus `max_df` is set to `.7`.\n",
    "- Since the books have been `pre-processed` in [00_create_texts_dicts_corpora.py](00_create_texts_dicts_corpora.py), there is no need to `tokenize` again. However, if any common `stop_words` are still present (due to issues in the text), they will be removed again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "        min_df=1,\n",
    "        max_df=.85,\n",
    "        stop_words=sw,\n",
    "        analyzer='word',\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "From Category 134\n",
      "###############\n",
      "TFIDF Books 0 - 7.\n",
      "TFIDF Books 7 - 13.\n",
      "TFIDF Books 13 - 23.\n",
      "TFIDF Books 23 - 33.\n",
      "TFIDF Books 33 - 43.\n",
      "TFIDF Books 43 - 48.\n",
      "###############\n",
      "From Category 135\n",
      "###############\n",
      "TFIDF Books 0 - 2.\n",
      "TFIDF Books 2 - 20.\n",
      "TFIDF Books 20 - 32.\n",
      "TFIDF Books 32 - 40.\n",
      "TFIDF Books 40 - 57.\n",
      "TFIDF Books 57 - 66.\n",
      "###############\n",
      "From Category 136\n",
      "###############\n",
      "TFIDF Books 0 - 2.\n",
      "TFIDF Books 2 - 10.\n",
      "TFIDF Books 10 - 25.\n",
      "TFIDF Books 25 - 35.\n",
      "TFIDF Books 35 - 51.\n",
      "TFIDF Books 51 - 60.\n",
      "###############\n",
      "From Category 137\n",
      "###############\n",
      "TFIDF Books 0 - 6.\n",
      "TFIDF Books 6 - 16.\n",
      "TFIDF Books 16 - 28.\n",
      "TFIDF Books 28 - 40.\n",
      "TFIDF Books 40 - 60.\n",
      "TFIDF Books 60 - 77.\n"
     ]
    }
   ],
   "source": [
    "all_tfidfs = []\n",
    "word_freqs = []\n",
    "for books,category_id in zip(all_books,categories):\n",
    "    print('###############\\nFrom Category {}\\n###############'.format(category_id))\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    time_range = time_ranges[int(category_id)]\n",
    "    school_tfidfs = []\n",
    "    \n",
    "    for time in range(0,len(time_range)):\n",
    "        end_time += time_range[time]\n",
    "        print('TFIDF Books {} - {}.'.format(start_time,end_time))\n",
    "        \n",
    "        bow = tfidf.fit_transform(books[start_time:end_time])\n",
    "        all_tfidfs.append(bow)\n",
    "        sum_words = bow.sum(axis=0)\n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        school_tfidfs.append(words_freq)\n",
    "        start_time += time_range[time]\n",
    "    word_freqs.append(school_tfidfs)\n",
    "    \n",
    "with open('../../data/all_word_freqs_85.json', 'w') as f:\n",
    "        json.dump(word_freqs,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
